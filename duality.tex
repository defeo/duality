\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,latexsym}
\usepackage{mathrsfs}
\usepackage[all]{xypic}

\newcommand{\cat}[1]{\mathscr{#1}}
\newcommand{\lcat}[1]{\mathbf{#1}}
\newcommand{\C}{\cat{C}}
\newcommand{\D}{\cat{D}}
\renewcommand{\L}{\cat{L}}
\newcommand{\comp}{\circ}
\newcommand{\size}[1]{\lVert#1\rVert}
\newcommand{\sizein}[1]{\size{#1}^\mathrm{in}}
\newcommand{\sizeout}[1]{\size{#1}_\mathrm{out}}
\DeclareMathOperator{\ob}{ob}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Id}{Id}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Complex}{\mathbb{C}}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow}
\DeclareMathOperator{\Time}{t}
\DeclareMathOperator{\Space}{s}
\DeclareMathOperator{\coTime}{co-t}
\DeclareMathOperator{\coSpace}{co-s}
\DeclareMathOperator{\Par}{Par}
\newcommand{\computes}{\vdash}

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{remark}[definition]{Remark}


\begin{document}
  \title{Tellegen's principle, dualities and applications}
  \author{Luca De Feo}
  \date{\today}
  
  \maketitle

  \begin{abstract}
    Tellegen's principle is a classical computer algebra result
    stating that ``from every \emph{linear algorithm} computing a
    linear application we can deduce another \emph{linear algorithm}
    computing the transpose application that uses \emph{about} the
    same space and time resources''.

    Tellegen's principle can be made precise in many ways, yielding
    theorems in the fields of computer algebra, algebraic complexity
    and electric networks. Here we give a Tellegen's theorem in the
    setting of category theory and we investigate its consequences.

    We also give some practical application of the principle and
    discuss the feasibility of a computer implementation of the
    principle.

    Our hope is to cover the whole subject from very abstract theory
    to extreme practice. This work is fully experimental and still in
    progress, so read carefully the footnotes!
  \end{abstract}


  \section{Category theory}

  A category is

  A sequence is, a morphism (isomorphism) of sequences is, the
  category of the sequences $\C_\leadsto$
  
  The skeleton of a category is

  A covariant functor is
  
  A contravariant functor is

  A natural transformation is

  An equivalence of categories is, a duality of categories is

  Examples (the category of finite dimensional free $A$-modules and
  its self-duality)

  
  \section{Additions to category theory
    \protect\footnote{This section is certainly the least elegant
      one. A category theoretical restatement of the concepts
      introduced here would be greatly welcome.}}

  We want to model computations that stay inside a category. We can
  think of the objects of our category as the data set of our
  computation (inputs, outputs and work registers) and the action of
  \emph{going through} an arrow will be interpreted as
  \emph{performing} the computation.
  
  Since we are studying complexities, we need some way of quantifying
  the time and the space used by a computation. Space is easy : we
  just want to attach to each object the information of how much space
  it takes to represent its generic element. This can be done by a
  function $\size{.}$ that associates a natural number to each object
  in a consistent way.

  \begin{definition}[Size function]
    Let $\C$ be a small category, $\size{.} : \ob(\C) \rightarrow \N$ is
    said to be a \emph{size function} of $\C$ if the condition
    \begin{equation}
      \label{eq:size}
      o_1 \text{ and } o_2 \text{ are isomorphic}
      \Rightarrow \size{o_1} = \size{o_2}
    \end{equation}
    holds for every two objects $o_1,o_2\in\ob(\C)$.
  \end{definition}
  
  Equivalently, a size function can be defined as a functor from the
  skeleton of $\C$ to the category $\N$ of finite ordinals (natural
  numbers) with all the functions, thus allowing us to consider
  categories that aren't small.\footnote{I'm confused about this
  definition. Though it might seem reasonable that isomorphic
  structures should always take the same number of bits to represent
  their objects, it seems to me that we're letting out of the door the
  possibility of redundant representations and, finally, a very
  important computer algebra technique: the ``change of
  representation''. After all, can't we relax condition
  \eqref{eq:size}?}

  To take time into account, we must decide which arrows correspond to
  \emph{atomic computations}, i.e. computations that cost a unit of
  time. We can do this by distinguishing some arrows.

  \begin{definition}[Language]
    Let $\C$ be a small category, a language $\L$ is a subset of
    $\hom(\C)$. It's elements are called
    \emph{instructions}\footnote{I'm trying to find a Yoneda-style way
    of defining this concept for locally small categories. My idea was
    : add an initial object $I$ to $\C$ and identify $\Hom(X,Y)$ in
    $\C$ to $\Hom\left(\Hom(I,I), \Hom(X,Y)\right)$ in $\lcat{Set}$,
    then a well chosen subcategory of $\lcat{Set}$ shall (hopefully)
    define $\L$. Does anyone want to help me formalise this? But see
    next note too.}\footnote{The last changes to this work have
    defined the category $\C_\ra$ of computations in $\C$. It turns
    out that it is a subcategory of $\C_\leadsto$, the category of
    finite sequences of $\C$. May it be possible to swap the order of
    this presentation and define the language as a subcategory of
    $\C_\leadsto$?}
    
    A language is said to be \emph{complete} if $(\ob(\C),\L)$ freely
    generates $\C$.
  \end{definition}

  Completeness says that $\L$ is powerful enough to perform every
  computation in $\C$; This is a nice, though not strictly necessary
  property. We are now ready to define a computation.

  \begin{definition}[Computation]
    Let $\C$ be a category and $\L$ a language of $\C$, a
    \emph{computation} is a finite sequence of $\C$ whose arrows all
    belong to $\L$. If $s$ is the source of the sequence and $t$ is
    the target, we note $C:s\ra t$ or simply $C$ for the
    computation ; we also set the following notation
    \begin{align*}
      &\sizein{C:s\ra t} = \size{s} \text{ ,}\\
      &\sizeout{C:s\ra t} = \size{t} \text{ .}
    \end{align*}
    
    Computations can be composed by the natural notion of composition
    of sequences, so if $C_1 : b\ra c$ and $C_2 : a\ra b$ are two
    computations, we note $C_1\comp C_2 : a\ra c$ for their
    composition.

    An arrow $f$ of $\C$ is said to be \emph{computable} if there
    exists a computation $C = (c_0, c_1,\ldots,c_n)$ such that $f =
    c_n\comp\ldots\comp c_1\comp c_0$, we note $C\computes f$.
  \end{definition}

  Sometimes the computation $C$ for a given $f$ will be understood and
  we'll note $f$ instead of $C$ by abuse of notation, so, for example,
  if $C_1\computes f_1$ and $C_2\computes f_2$ we'll note $f_1\comp
  f_2$ instead of $C_1\comp C_2$. When we'll draw diagrams, we'll use
  continue lines for instructions and dashed lines for general arrows,
  like on figure \ref{fig:comp}.
  \begin{figure}[!h]
    \[\xymatrix{
      & \ar[r]\ar@{}[dr]|{C_2} & \ar[dr] &
      & \ar[r]\ar@{}[dr]|{C_1} & \ar[dr] \\
      a \ar[ur]\ar@{-->}[rrr]_{f_2} &&& b
      \ar[ur]\ar@{-->}[rrr]_{f_1} &&& c
    }\]
    \caption{\label{fig:comp}A diagram illustrating computations,
    computable arrows and composition.}
  \end{figure}
  
  We now have all the elements to study computations in a
  category. We'll study a category together with a size function and a
  language over it. We'll call it a \emph{computational category} and
  we'll write $(\C, \size{}, \L)$, or simply $\C$ when $\size{}$ and
  $\L$ are understood.

  In this setting, we can measure computations. As in computational
  complexity, two measures will be relevant : time and space.

  \begin{definition}[Space and time cost]
    Let $(\C, \size{}, \L)$ be a computational category and let $C$ be
    the computation
    \[o_0 \ra o_1 \ra \cdots \ra o_n \text{ .}\]
    We define the two quantities
    \begin{align*}
      \Time(C) &= n \text{,} \\
      \Space(C) &= \max_{0\le i \le n} \size{o_i} \text{ ,}
    \end{align*}
    and we call them respectively the \emph{time-cost} and the
    \emph{space-cost} of $C$.
  \end{definition}

  Considering single computations is enough to state a weak form of
  Tellegen's theorem. From a computer science point of view, though, a
  computation is still quite far from a ``real world'' algorithm. What
  we need is to be able to talk about more than a computation at a
  time.

  \begin{definition}[Morphism of computations]
    Let $C$ and $C'$ be two computations, a \emph{morphism of
      computations} $f:C\ra C'$ is a morphism of sequences from $C$ to
    $C'$. An \emph{isomorphism of computations} is a morphism that is
    an isomorphism of sequences. Two computations are said to be
    \emph{isomorphic} if there is an isomorphism of computations
    between them. Two instructions are said to be isomorphic if they
    are isomorphic as computations.

    The category of the computations of $\C$, noted $\C_\ra$, is the
    subcategory of $\C_\leadsto$ whose objects are the computations
    and whose arrows are the morphisms of computations.
  \end{definition}

  Using diagrams, a morphism of computations looks like this
  \[\xymatrix{
    c_0 \ar[r]^{i_0} \ar@{-->}[d]^{f_0} & c_1 \ar@{-->}[d]^{f_1} \ar@{.}[rr] && c_n\ar@{-->}[d]^{f_n}\\
    c_0' \ar[r]^{i_0'} & c_1' \ar@{.}[rr] && c_n'
  }\]
  Note that the $f_i$'s are not necessarily computations, so that even
  if we have such a diagram, it might be computationally hard (or
  impossible) to go from $c_0$ to $c_n$.
  
  By means of this definition, a computation $C:s\ra t$ defines a
  class\footnote{Is it true that $\C$ locally small $\Leftrightarrow
    C_\equiv$ is a set ?}, noted $C_\equiv$, of isomorphic
  computations. Obviously, two isomorphic computations have the same
  time-cost, space-cost and sizes, so that $\Time(C_\equiv)$,
  $\Space(C_\equiv)$, $\sizein{C_\equiv}$ and $\sizeout{C}$ are well
  defined. From now on we'll use computations as representatives for
  their isomorphism class and we'll abuse of our notation by writing
  $C$ instead of $C_\equiv$.
  
%%   The next problem to sort out is the fact that the definition of a
%%   language is so weak that it allows us to do computationally
%%   impossible things.
  
%%   \begin{definition}[Soundness]
%%     A language is said to be \emph{sound} if for every instruction
%%     $a:s\ra t$ and for every $s'$ isomorphic to $s$ there is an
%%     instruction $b:s'\ra t'$ isomorphic to $a$.
%%   \end{definition}

%%   From a practical point of view, two isomorphic objects are
%%   indistinguishable, so that a non-sound language would give us an
%%   unnatural way of distinguishing two such objects. That's why we'll
%%   always require for languages to be sound.

  An algorithm is more than just an isomorphism class of
  computations. When a computer scientist writes an algorithm, he
  chooses to perform a different computation depending on some values
  in a parameter space. For example, depending on the input size he
  may chose to iterate some instruction a certain number of
  times. Generalising this idea, we get the following definition.

  \begin{definition}[Algorithm]
    Let $\C$ be a computational category and let $\C_\ra$ be its
    category of computations. Let $\Par$ be a recursively
    enumerable set and let $\lcat{Par}$ be the discrete category over
    it. An \emph{algorithm} $A$ is a functor from $\lcat{Par}$ to
    $\C_\ra$.

    $\Par$ is called the parameter space of $A$.
  \end{definition}

  Since $\Par$ is recursively enumerable, $A$ defines a recursively
  enumerable set of (isomorphism classes of) computations. It can then
  be viewed as a function from $\Par$ to some sets of relevant
  properties about computations, like in the following definition.

  \begin{definition}[Space and time complexity]
    Given an algorithm $A:\Par\ra\C_\ra$, its \emph{time complexity}
    is the partial function ${\Time_A:\Par\ra\N}$ given by
    \begin{equation*}
      \Time_A(x) = \Time(A(x)) \text{,}
    \end{equation*}
    Its \emph{space complexity} $\Space_A$ is defined in the same way.
  \end{definition}

  When $\Par$ is $\N^n$ for some integer $n$, we find that the
  complexities are functions $\N^n\ra\N$ as we are used to. But in
  general nothing prevents us from taking for $\Par$ the set of prime
  numbers or even the set of solvable Diophantine equations.

  \paragraph{Example}
  As an example, the category $\lcat{FMod}_R$ of finite dimensional
  free $R$-modules can be given a structure of computational
  category. Following \cite{BLS03}, we give the following operators
  \begin{align*}
    +_1 : R^2 &\ra R^2         &   +_2 : R^2&\ra R^2       &  *_a : R&\ra R\\
         (p,q)&\mapsto(p+q,q)  &      (p,q)&\mapsto(p,p+q) &       p&\mapsto ap
  \end{align*}
  In order to be consistent with our setting, we add a projection and
  an injection
  \begin{align*}
    \pi : R&\ra 0     &  \iota : 0&\ra R   \\
          p&\mapsto0  &          0&\mapsto0
  \end{align*}
  Then we define the language $\L$ over $\lcat{FMod}_R$ as 
  \begin{equation}
    \label{eq:linlang}
    \L = \Bigl\{\Id_n\times\mathrm{op}\times\Id_m \Bigl\lvert
    n,m\in\N, \mathrm{op}\in\{+_1,+_2,*_a,\pi,\iota\;|\;a\in R\} \Bigr\}
    \text{.}
  \end{equation}
  The completeness of the language is not obvious and I'll prove it
  one day.

  The space function is exactly what one would expect it to be :
  \begin{equation}
    \size{R^n} = n \text{.}
  \end{equation}
  
  Let $\ell_n$ be the linear form $R^n\mapsto R$ that maps all the
  vectors of the canonical base to $1$, imagine that we want an
  algorithm that given any element in $v\in R^n$ computes $\ell_n(v)$,
  for any $n$.

  The actual action of such an algorithm is determined by the size $n$
  of the input vector, hence a suitable parameter space is $\N$. Our
  algorithm $A:\N\ra\C_\ra$ sends an integer $n$ over the computation
  of $\ell_n$. The fact that a computation for $\ell_n$ really exists
  is showed by the following diagram
  \[\xymatrix{
    R^n\ar[r]^{+_2\times{\Id_{n-2}}} & R^n\ar[r]^{\pi\times{\Id_{n-1}}} &
    R^{n-1} \ar@{.}[rr] && R
  }\]
  
  With a little more insight, we see that we have just defined the
  algorithm given by the following C-like syntax
  \begin{center}
    \begin{minipage}{0.7\textwidth}
\begin{verbatim}
for i = 0 to n-2 do
   a[1] = a[1] + a[2]
   a = &(a[1])
end for
\end{verbatim}
    \end{minipage}
  \end{center}

  Space and time complexities are then functions $\N\ra\N$. As we
  would expect, $\Space_A$ is the function $n$, while $\Time_A$ is
  $2n$.


  \paragraph{Exemple}
  Consider the category made of two objects and two arrows that are
  both in $\L$.
  \[\xymatrix{ Y \ar@(dl,ul)^{\id_Y} & N \ar@(ur,dr)^{\id_N} }\]
  Computations in $\C_\ra$ are all the sequences of finite length that
  stay either in $Y$ or in $N$. Let $\Par$ be a parameter space, such
  a category can be used to state any decision problem in $\Par$.

  As an exemple, let $T$ be the set of all Turing machines over an
  alphabet $\Sigma$, we chose $T\times\Sigma^\ast$ as our parameter
  space and we define the algorithm $A:T\times\Sigma^\ast\ra\C_\ra$ in
  the following way
  \begin{equation*}
    A(t,s) = \begin{cases}
      \id_Y & \text{if $t(s)$ halts,}\\
      \id_N & \text{if $t(s)$ loops forever.}
    \end{cases}
  \end{equation*}
  

  \section{Tellegen's theorem}

  Tellegen's theorem talks about transformations of algorithms. In
  category theory transforming one category into another is done
  through the use of functors.

  \begin{definition}[Tellegen functor]
    Let $(\C,\size{}_{\C},\L)$ and $(\D,\size{}_{\D},\cat{M})$ be two
    computational categories. A (covariant or contravariant) functor
    $F:\C\ra\D$ is a \emph{Tellegen functor} if $F(\L)\subset\cat{M}$.
  \end{definition}

  Tellegen functors preserve computations, so if $C$ is a computation
  of $\C$, we note $F(C)$ its image in $D$, then it's evident that
  $\Time(F(C)) = \Time(C)$. Functors preserve isomorphisms too, so
  $F(C_\equiv) \subset F(C)_\equiv$.


  
  \section{Tellegen's principle into practice}
  \label{sec:practice}
  
  \section{Application}


  
  \begin{thebibliography}{1}
    
  \bibitem{BCS}P.~Bürgisser, M.~Clausen \& M.~A.~Shokrollahi
    \emph{Algebraic Complexity Theory}.
    Springer, 1997.
    
  \bibitem{BLS03}A.~Bostan, G.~Lecerf \& E.~Schost,
    Tellegen's Principle into Practice.
    \emph{Proceedings of ISAAC 2003}.

  \bibitem{WIKI}Wikipedia, The free encycolpedia.
  
  \end{thebibliography}

\end{document}


% Local Variables:
% mode:flyspell
% ispell-local-dictionary:"british"
% End:

% LocalWords:  Tellegen Tellegen's covariant functor contravariant injective
% LocalWords:  coalgorithm cocomplexity bialgorithm subcategory
