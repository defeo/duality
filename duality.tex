\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,latexsym}
\usepackage{mathrsfs}
\usepackage[all]{xypic}

\newcommand{\cat}[1]{\mathscr{#1}}
\newcommand{\lcat}[1]{\mathbf{#1}}
\newcommand{\C}{\cat{C}}
\newcommand{\D}{\cat{D}}
\renewcommand{\L}{\cat{L}}
\newcommand{\comp}{\circ}
\newcommand{\size}[1]{\lVert#1\rVert}
\newcommand{\sizein}[1]{\size{#1}^\mathrm{in}}
\newcommand{\sizeout}[1]{\size{#1}_\mathrm{out}}
\DeclareMathOperator{\ob}{ob}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Id}{Id}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Complex}{\mathbb{C}}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow}
\DeclareMathOperator{\Time}{t}
\DeclareMathOperator{\Space}{s}
\DeclareMathOperator{\coTime}{co-t}
\DeclareMathOperator{\coSpace}{co-s}
\DeclareMathOperator{\Par}{Par}
\newcommand{\computes}{\vdash}

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{remark}[definition]{Remark}


\begin{document}
  \title{Tellegen's principle, dualities and applications}
  \author{Luca De Feo}
  \date{\today}
  
  \maketitle

  \begin{abstract}
    Tellegen's principle is a classical computer algebra result
    stating that ``from every \emph{linear algorithm} computing a
    linear application we can deduce another \emph{linear algorithm}
    computing the transpose application using \emph{about} the same
    space and time resources''.

    Tellegen's principle can be made precise in many ways, yielding
    theorems in the fields of computer algebra, algebraic complexity
    and electric networks. Here we give a Tellegen's theorem in the
    setting of category theory and we investigate its consequences.
    Finally, we discuss the feasibility of a computer implementation
    of the principle.

    Our hope is to cover the whole subject from very abstract theory
    to extreme practice. This work is fully experimental and still in
    progress, so read carefully the footnotes!
  \end{abstract}


  \section{Category theory}

  A category is

  A sequence is, a morphism (isomorphism) of sequences is, the
  category of the sequences $\C_\leadsto$
  
  The skeleton of a category is

  A covariant functor is
  
  A contravariant functor is

  A natural transformation is

  An equivalence of categories is, a duality of categories is

  Examples (the category of finite dimensional free $A$-modules and
  its self-duality)

  
  \section{Additions to category theory
    \protect\footnote{This section is certainly the least elegant
      one. A category theoretical restatement of the concepts
      introduced here would be greatly welcome.}}

  We want to model computations that stay inside a category. We can
  think of the objects of our category as the data set of our
  computation (inputs, outputs and work registers) and the action of
  \emph{going through} an arrow will be interpreted as
  \emph{performing} the computation.
  
  Since we are studying complexities, we need some way of quantifying
  the time and the space used by a computation. Space is easy : we
  just want to attach to each object the information of how much space
  it takes to represent its generic element. This can be done by a
  function $\size{.}$ that associates a natural number to each object
  in a consistent way.

  \begin{definition}[Size function]
    Let $\C$ be a small category, $\size{.} : \ob(\C) \rightarrow \N$ is
    said to be a \emph{size function} of $\C$ if the condition
    \begin{equation}
      \label{eq:size}
      o_1 \text{ and } o_2 \text{ are isomorphic}
      \Rightarrow \size{o_1} = \size{o_2}
    \end{equation}
    holds for every two objects $o_1,o_2\in\ob(\C)$.
  \end{definition}
  
  Equivalently, a size function can be defined as a functor from the
  skeleton of $\C$ to the category $\N$ of finite ordinals (natural
  numbers) with all the functions, thus allowing us to consider
  categories that aren't small.\footnote{I'm confused about this
  definition. Though it might seem reasonable that isomorphic
  structures should always take the same number of bits to represent
  their objects, it seems to me that we're letting out of the door the
  possibility of redundant representations and, finally, a very
  important computer algebra technique: the ``change of
  representation''. After all, can't we relax condition
  \eqref{eq:size}?}

  To take time into account, we must decide which arrows correspond to
  \emph{atomic computations}, i.e. computations that cost a unit of
  time. We can do this by distinguishing some arrows.

  \begin{definition}[Language]
    Let $\C$ be a small category, a language $\L$ is a subset of
    $\hom(\C)$. It's elements are called
    \emph{instructions}\footnote{I'm trying to find a Yoneda-style way
    of defining this concept for locally small categories. My idea was
    : add an initial object $I$ to $\C$ and identify $\Hom(X,Y)$ in
    $\C$ to $\Hom\left(\Hom(I,I), \Hom(X,Y)\right)$ in $\lcat{Set}$,
    then a well chosen subcategory of $\lcat{Set}$ shall (hopefully)
    define $\L$. Does anyone want to help me formalise this? But see
    next note too.}\footnote{The last changes to this work have
    defined the category $\C_\ra$ of computations in $\C$. It turns
    out that it is a subcategory of $\C_\leadsto$, the category of
    finite sequences of $\C$. May it be possible to swap the order of
    this presentation and define the language as a subcategory of
    $\C_\leadsto$?}
    
    A language is said to be \emph{complete} if $(\ob(\C),\L)$ freely
    generates $\C$.
  \end{definition}

  Completeness says that $\L$ is powerful enough to perform every
  computation in $\C$; This is a nice, though not strictly necessary
  property. We are now ready to define a computation.

  \begin{definition}[Computation]
    Let $\C$ be a category and $\L$ a language of $\C$, a
    \emph{computation} is a finite sequence of $\C$ whose arrows all
    belong to $\L$. If $s$ is the source of the sequence and $t$ is
    the target, we note $C:s\ra t$ or simply $C$ for the
    computation ; we also set the following notation
    \begin{align*}
      &\sizein{C:s\ra t} = \size{s} \text{ ,}\\
      &\sizeout{C:s\ra t} = \size{t} \text{ .}
    \end{align*}
    
    Computations can be composed by the natural notion of composition
    of sequences, so if $C_1 : b\ra c$ and $C_2 : a\ra b$ are two
    computations, we note $C_1\comp C_2 : a\ra c$ for their
    composition.

    An arrow $f$ of $\C$ is said to be \emph{computable} if there
    exists a computation $C = (c_0, c_1,\ldots,c_n)$ such that $f =
    c_n\comp\ldots\comp c_1\comp c_0$, we note $C\computes f$.
  \end{definition}

  Sometimes the computation $C$ for a given $f$ will be understood and
  we'll note $f$ instead of $C$ by abuse of notation, so, for example,
  if $C_1\computes f_1$ and $C_2\computes f_2$ we'll note $f_1\comp
  f_2$ instead of $C_1\comp C_2$. When we'll draw diagrams, we'll use
  continue lines for instructions and dashed lines for general arrows,
  like on figure \ref{fig:comp}.
  \begin{figure}[!h]
    \[\xymatrix{
      & \ar[r]\ar@{}[dr]|{C_2} & \ar[dr] &
      & \ar[r]\ar@{}[dr]|{C_1} & \ar[dr] \\
      a \ar[ur]\ar@{-->}[rrr]_{f_2} &&& b
      \ar[ur]\ar@{-->}[rrr]_{f_1} &&& c
    }\]
    \caption{\label{fig:comp}A diagram illustrating computations,
    computable arrows and composition.}
  \end{figure}
  
  We now have all the elements to study computations in a
  category. We'll study a category together with a size function and a
  language over it. We'll call it a \emph{computational category} and
  we'll write $(\C, \size{}, \L)$, or simply $\C$ when $\size{}$ and
  $\L$ are understood.

  In this setting, we can measure computations. As in computational
  complexity, two measures will be relevant : time and space.

  \begin{definition}[Space and time cost]
    Let $(\C, \size{}, \L)$ be a computational category and let $C$ be
    the computation
    \[o_0 \ra o_1 \ra \cdots \ra o_n \text{ .}\]
    We define the two quantities
    \begin{align*}
      \Time(C) &= n \text{,} \\
      \Space(C) &= \max_{0\le i \le n} \size{o_i} \text{ ,}
    \end{align*}
    and we call them respectively the \emph{time-cost} and the
    \emph{space-cost} of $C$.
  \end{definition}

  Considering single computations is enough to state a weak form of
  Tellegen's theorem. From a computer science point of view, though, a
  computation is still quite far from a ``real world'' algorithm. What
  we need is to be able to talk about more than a computation at a
  time.

  \begin{definition}[Morphism of computations]
    Let $C$ and $C'$ be two computations, a \emph{morphism of
      computations} $f:C\ra C'$ is a morphism of sequences from $C$ to
    $C'$. An \emph{isomorphism of computations} is a morphism that is
    an isomorphism of sequences. Two computations are said to be
    \emph{isomorphic} if there is an isomorphism of computations
    between them. Two instructions are said to be isomorphic if they
    are isomorphic as computations.

    The category of the computations of $\C$, noted $\C_\ra$, is the
    subcategory of $\C_\leadsto$ whose objects are the computations
    and whose arrows are the morphisms of computations.
  \end{definition}

  Using diagrams, a morphism of computations looks like this
  \[\xymatrix{
    c_0 \ar[r]^{i_0} \ar@{-->}[d]^{f_0} & c_1 \ar@{-->}[d]^{f_1} \ar@{.}[rr] && c_n\ar@{-->}[d]^{f_n}\\
    c_0' \ar[r]^{i_0'} & c_1' \ar@{.}[rr] && c_n'
  }\]
  Note that the $f_i$'s are not necessarily computations, so that even
  if we have such a diagram, it might be computationally hard (or
  impossible) to go from $c_0$ to $c_n$.
  
  By means of this definition, a computation $C:s\ra t$ defines a
  class\footnote{Is it true that $\C$ locally small $\Leftrightarrow
    C_\equiv$ is a set ?}, noted $C_\equiv$, of isomorphic
  computations. Obviously, two isomorphic computations have the same
  time-cost, space-cost and sizes, so that $\Time(C_\equiv)$,
  $\Space(C_\equiv)$, $\sizein{C_\equiv}$ and $\sizeout{C}$ are well
  defined. From now on we'll use computations as representatives for
  their isomorphism class and we'll abuse of our notation by writing
  $C$ instead of $C_\equiv$.
  
%%   The next problem to sort out is the fact that the definition of a
%%   language is so weak that it allows us to do computationally
%%   impossible things.
  
%%   \begin{definition}[Soundness]
%%     A language is said to be \emph{sound} if for every instruction
%%     $a:s\ra t$ and for every $s'$ isomorphic to $s$ there is an
%%     instruction $b:s'\ra t'$ isomorphic to $a$.
%%   \end{definition}

%%   From a practical point of view, two isomorphic objects are
%%   indistinguishable, so that a non-sound language would give us an
%%   unnatural way of distinguishing two such objects. That's why we'll
%%   always require for languages to be sound.

  An algorithm is more than just an isomorphism class of
  computations. When a computer scientist writes an algorithm, he
  chooses to perform a different computation depending on some values
  in a parameter space. For example, depending on the input size he
  may chose to iterate some instruction a certain number of
  times. Generalising this idea, we get the following definition.

  \begin{definition}[Algorithm]
    Let $\C$ be a computational category and let $\C_\ra$ be its
    category of computations. Let $\Par$ be a recursively enumerable
    set and let $\lcat{Par}$ be the discrete category over it. An
    \emph{algorithm} $A$ is a functor from $\lcat{Par}$ to
    $\C_\ra$.\footnote{I'm not quite sure this definition is strong
      enough. The idea is to give a model as strong as any other
      computational model, such as Turing machines or computer
      programs. I'm afraid, though not sure, that this definition
      allows to write decision procedures for problems that are not
      recursively enumerable (i.e., semi-decidable), thus making this
      model strictly stronger than Turing machines. I see two ways to
      fix this :\begin{itemize}
      \item I ignore it, but maybe any subset of a r.e. set is r.e. In
        this case, there's nothing to do and the definition is good;
      \item r.e subsets of $\Par$ are the open sets of a topology of
        $\Par$. Asking for some sort of continuity (perhaps with a
        discrete topology over $\ob(\C_\ra)$), shall fix the problem.
    \end{itemize}}

    $\Par$ is called the parameter space of $A$.
  \end{definition}

  Since $\Par$ is recursively enumerable, $A$ defines a recursively
  enumerable set of (isomorphism classes of) computations. It can then
  be viewed as a function from $\Par$ to some sets of relevant
  properties about computations, like in the following definition.

  \begin{definition}[Space and time complexity]
    Given an algorithm $A:\Par\ra\C_\ra$, its \emph{time complexity}
    is the partial function ${\Time_A:\Par\ra\N}$ given by
    \begin{equation*}
      \Time_A(x) = \Time(A(x)) \text{,}
    \end{equation*}
    Its \emph{space complexity} $\Space_A$ is defined in the same way.
  \end{definition}

  When $\Par$ is $\N^n$ for some integer $n$, we find that the
  complexities are functions $\N^n\ra\N$ as we are used to. But in
  general nothing prevents us from taking for $\Par$ the set of prime
  numbers or even the set of solvable Diophantine equations.

  \paragraph{Example}
  As an example, the category $\lcat{FMod}_R$ of finite dimensional
  free $R$-modules can be given a structure of computational
  category. Following \cite{BLS03}, we give the following operators
  \begin{align*}
    +_1 : R^2 &\ra R^2         &   +_2 : R^2&\ra R^2       &  *_a : R&\ra R\\
         (p,q)&\mapsto(p+q,q)  &      (p,q)&\mapsto(p,p+q) &       p&\mapsto ap
  \end{align*}
  In order to be consistent with our setting, we add a projection and
  an injection
  \begin{align*}
    \pi : R&\ra 0     &  \iota : 0&\ra R   \\
          p&\mapsto0  &          0&\mapsto0
  \end{align*}
  Then we define the language $\L$ over $\lcat{FMod}_R$ as 
  \begin{equation}
    \label{eq:linlang}
    \L = \Bigl\{\Id_n\times\mathrm{op}\times\Id_m \Bigl\lvert
    n,m\in\N, \mathrm{op}\in\{+_1,+_2,*_a,\pi,\iota\;|\;a\in R\} \Bigr\}
    \text{.}
  \end{equation}
  The completeness of the language is not obvious and I'll prove it
  one day.

  The space function is exactly what one would expect it to be :
  \begin{equation}
    \size{R^n} = n \text{.}
  \end{equation}
  
  Let $\ell_n$ be the linear form $R^n\mapsto R$ that maps all the
  vectors of the canonical base to $1$, imagine that we want an
  algorithm that given any element in $v\in R^n$ computes $\ell_n(v)$,
  for any $n$.

  The actual action of such an algorithm is determined by the size $n$
  of the input vector, hence a suitable parameter space is $\N$. Our
  algorithm $A:\N\ra\C_\ra$ sends an integer $n$ over the computation
  of $\ell_n$. The fact that a computation for $\ell_n$ really exists
  is showed by the following diagram
  \[\xymatrix{
    R^n\ar[r]^{+_2\times{\Id_{n-2}}} & R^n\ar[r]^{\pi\times{\Id_{n-1}}} &
    R^{n-1} \ar@{.}[rr] && R
  }\]
  
  With a little more insight, we see that we have just defined the
  algorithm given by the following C-like syntax
  \begin{center}
    \begin{minipage}{0.7\textwidth}
\begin{verbatim}
for i = 0 to n-2 do
   a[1] = a[0] + a[1]
   a = &(a[1])
end for
\end{verbatim}
    \end{minipage}
  \end{center}

  Space and time complexities are then functions $\N\ra\N$. As we
  would expect, $\Space_A$ is the function $n$, while $\Time_A$ is
  $2n$.


  \paragraph{Example}
  Consider the category made of two objects and two arrows that are
  both in $\L$.
  \[\xymatrix{ Y \ar@(dl,ul)^{\id_Y} & N \ar@(ur,dr)^{\id_N} }\]
  Computations in $\C_\ra$ are all the sequences of finite length that
  stay either in $Y$ or in $N$. Let $\Par$ be a parameter space, such
  a category can be used to state any decision problem in $\Par$.

  As an example, let $T$ be the set of all Turing machines over an
  alphabet $\Sigma$. We chose $T\times\Sigma^\ast$ as our parameter
  space and we define the algorithm ${A:T\times\Sigma^\ast\ra\C_\ra}$ in
  the following way
  \begin{equation*}
    A(t,s) = \begin{cases}
      \id_Y & \text{if $t(s)$ halts,}\\
      \id_N & \text{if $t(s)$ loops forever.}
    \end{cases}
  \end{equation*}
  
  It is evident that $\Time_A$ is the constant function $1$, while the
  algorithm is a decision procedure for the famous halting problem,
  which is known to be semi-decidable. The paradox is due to the fact
  that the time complexity function we defined accounts only for what
  happens inside the category, thus allowing us to hide a lot of
  complexity (potentially all of it) inside the parameter space.

  As we'll discuss in section \ref{sec:practice}, this phenomenon is
  fundamental to understand how automatic transposition of computer
  programs works.


  \section{Tellegen's theorem}

  Tellegen's theorem talks about transformations of algorithms. In
  category theory transforming one category into another is done
  through the use of functors.

  \begin{definition}[Tellegen functor]
    Let $(\C,\size{}_{\C},\L)$ and $(\D,\size{}_{\D},\cat{M})$ be two
    computational categories. A (covariant or contravariant) functor
    $F:\C\ra\D$ is a \emph{Tellegen functor} if $F(\L)\subset\cat{M}$.
  \end{definition}

  A functor $F:\C\ra\D$ induces a functor
  $F_\leadsto:\C_\leadsto\ra\D_\leadsto$; moreover, Tellegen functors
  induce a functor $F_\ra:\C_\ra\ra\D_\ra$. If $C$ is a computation,
  by abuse of notation we'll note $F(C)$ instead of $F_\ra(C)$ for its
  image in $\D_\ra$. It is evident that $\Time(F(C)) = \Time(C)$ and,
  since functors preserve isomorphisms, that $F(C_\equiv) \subset
  F(C)_\equiv$. Space complexity is not preserved in general by a
  Tellegen functor, but suppose that we know a bound $B:\N\ra\N$ such
  that $\size{F(C)} \le B(n)$ for every computation $C$ such that
  $\size{C} = n$. We'll note $F\le_{\Space} B$ if $B$ is such a
  bound. We now have all the elements to state Tellegen's theorem.

  \begin{theorem}[Tellegen's theorem]
    Let $F:\C\ra\D$ be a Tellegen functor, let $\Par$ be a parameter
    space and let $A:\Par\ra\C_\ra$ be an algorithm. The functor
    $F\comp A$ is an algorithm for $\D$, noted $F(A)$. Its time
    complexity $\Time_{F(A)}$ is the same as $\Time_A$. Moreover, if
    $F\le_{\Space} B$, then $\Space_{F(A)} = B\comp\Space_A$.
  \end{theorem}

  The most useful cases of Tellegen's theorem are when $\C$ and $\D$
  are two equivalent (or dual) categories and the two functors
  expressing the equivalence (or duality) are indeed Tellegen's
  functors. In this case every algorithm in $\C$ can be moved to an
  algorithm in $\D$ and vice versa, so that any optimisation to an
  algorithm can be done in any of the two categories and then brought
  to the other. If moreover $\Id\le_{\Space}F\le_{\Space}\Id$, then
  moving algorithms between $\C$ and $\D$ guarantees that there's no
  loss of space efficiency.

  \paragraph{Example}
  Consider the category $\lcat{FMod}_R$ with the computational
  structure we gave in the last section. There is a well known
  self-duality over $\lcat{FMod}$ given by the functor $T$ that
  associates to every linear applications its transpose; notice that
  $T$ is its own inverse. By noticing the following equalities
  \[T(+_1) = +_2 \qquad T(*_a) = *_a \qquad T(\pi) = \iota \qquad T(\Id^n) = \Id^n\]
  we conclude that $T$ is a Tellegen functor and we easily see that it
  preserves space complexities. This gives us the classical
  formulation of Tellegen's theorem \cite{BCS,BLS03}.

  \paragraph{Example}
  Consider the category $\cat{Q}$ whose objects are the vector spaces
  $\left(\Complex^2\right)^{\otimes n}$ for any integer $n$ and whose
  arrows are all the unitary\footnote{Recall that $U$ unitary means
    $U$ invertible and $U^\ast=U^{-1}$.} transformations.
  
  This category is widely used in quantum computing, in fact a quantum
  circuit is defined exactly as a computation in this category. The
  choice of the language may vary depending on the particular
  presentation of the model, but a pretty common one is given by the
  following operators
  \begin{equation*}
    H = \frac{1}{\sqrt{2}}\begin{pmatrix}
      1 & -1 \\ -1 & 1
    \end{pmatrix},
    \qquad
    R = \begin{pmatrix}
      1 & 0 \\ 0 & e^{2\pi i }
    \end{pmatrix},
    \qquad
    CNOT = \begin{pmatrix}
      1 & 0 & 0 & 0\\
      0 & 1 & 0 & 0\\
      0 & 0 & 0 & 1\\
      0 & 0 & 1 & 0
    \end{pmatrix}.
  \end{equation*}
  The language is then defined as
  \begin{equation*}
    \L = \Bigl\{\Id_2^{\otimes n}\otimes\mathrm{op}\otimes\Id_2^{\otimes m}\Bigl\lvert
    n,m\in\N, \mathrm{op}\in\{H,R,R^\ast,CNOT\} \Bigr\}
    \text{.}
  \end{equation*}
  
  The size function is classically defined as
  $\size{(\Complex^2)^{\otimes n}} = n$, that is the number of
  \emph{qubits} the circuit operates on. Observe that since unitaries
  are invertible, they are auto-morphisms of the objects, then
  $\Space(C) = \sizein{C} = \sizeout{C}$ for any computation.

  This category has a well known self-duality given by the functor
  $\ast$ that sends any unitary on its inverse\footnote{Recall that
    inverse is the same thing as adjoint for unitaries. This makes the
    functor $\ast$ very similar to the functor $T$ of the previous
    example.}. It is easy to check that every element of $\L$ is its
  own inverse, except for $R^{-1} = R^\ast$, thus making $\ast$ a
  Tellegen functor (that is its own inverse).

  In this case Tellegen's theorem tells us a very well known axiom of
  quantum computing : ``Every computation is invertible. The inverse
  computation is done by reversing the circuit''.

  These two examples are the only well known applications of
  Tellegen's principle I'm aware of. Anyway, there's tons of well
  known category equivalences and dualities out there, that have been
  studied by mathematicians for centuries now. One may then ask if any
  of these equivalences can be turned into an useful Tellegen
  equivalence allowing us to apply transformation techniques to well
  known algorithms.
  

  \section{Tellegen's principle into practice}
  \label{sec:practice}
  In this section we are going to restrict ourself to Tellegen's
  theorem in the case $\C = \lcat{FMod}_R$. All the discussion will be
  general, though, and its ideas can be applied to the general
  Tellegen's theorem.

  When it comes down to really use Tellegen's principle to transform a
  computer program we've written into its transpose, it quickly comes
  up to a huge headache.

  The problem is that when we write a program we rarely code just one
  computation. We use instead a lot of \emph{non-linear} operators
  such as \verb|if|'s, \verb|while|'s and \verb|for|'s in order to
  synthesise the whole algorithm $A:\Par\ra\C_\ra$ into one single
  piece of code. The added \emph{non-linear} complexity required to
  express the mapping from $\Par$ to $\C_\ra$ is thus mixed together
  with the \emph{linear} operations that are indeed instructions of
  $\C$.

  On the other hand, the representation of a program as an algorithm
  $A:\Par\ra\C$ is of course not unique. In fact, any computer program
  can be viewed as a function on bit-strings
  $f:\{0,1\}^\ast\ra\{0,1\}^\ast$, so that it can be given a
  description as an algorithm $A:\{0,1\}^\ast\ra\lcat{FMod}_{\Z\ra}$,
  where any input $i$ is mapped to $*_a$ where $a$ is the integer
  whose binary representation is $f(i)$. With a bit more work, we can
  give a similar description as an algorithm
  $A:\{0,1\}^\ast\ra\lcat{FMod}_{R\ra}$ for any ring $R$.\footnote{The
    idea is that if $R$ has no torsion, then it contains $\Z$ and the
    same construction can be performed. Otherwise, let $r$ be its
    torsion and let $a$ be the integer whose binary expansion is
    $f(i)$, write $a$ in base $r$ as \[a = \sum a_ir^i\text{,}\] then
    map $i$ over $*_{a_0}\comp*_{a_1}\comp\cdots\comp_{a_n}$. The time
    complexity of such an algorithm is $\lceil\log_ra\rceil$.}

  Hence, the problem of \emph{automatic transposition} presents two
  complementary difficulties. One one hand, find the parameter space
  that gives the least complexity for the mapping $\Par\ra\C_\ra$, on
  the other hand split the code in two sets of instructions : those
  that are linear for the chosen mapping (and thus need to be
  transposed) and those that are not.

  Consider again this example that computes the linear form $\ell_n$
  \begin{center}
    \begin{minipage}{0.7\textwidth}
\begin{verbatim}
for i = 0 to n-2 do
   a[i+1] = a[i] + a[i+1]
   a[i] = 0
end for
\end{verbatim}
    \end{minipage}
  \end{center}
  by analysing the code we can see that a non-linear test is done on
  $n$ (the test for halting the loop), thus $n$ is a good thing to put
  in the parameter space, so let's say that our parameter space is
  $\N$.
  
  We now have to understand what happens for a fixed $n$. The
  instructions that are performed in this case are
  \begin{center}
    \begin{minipage}{0.7\textwidth}
\begin{verbatim}
   a[1] = a[0] + a[1]
   a[0] = 0
   a[2] = a[1] + a[2]
   a[1] = 0
   ...
   a[n-1] = a[n-2] + a[n-1]
   a[n-2] = 0
\end{verbatim}
    \end{minipage}
  \end{center}
  Recall that $T(*_0) = *_0$ and $T(+_1) = +_2$, then the
  transposition of this code gives
  \begin{center}
    \begin{minipage}{0.7\textwidth}
\begin{verbatim}
   a[n-2] = 0
   a[n-2] = a[n-2] + a[n-1]
   ...
   a[1] = 0
   a[1] = a[1] + a[2]
   a[0] = 0
   a[0] = a[0] + a[1]
\end{verbatim}
    \end{minipage}
  \end{center}
  Putting again things together we have
  \begin{center}
    \begin{minipage}{0.7\textwidth}
\begin{verbatim}
for i = n-2 to 0 do
   a[i] = 0
   a[i] = a[i] + a[i+1]
end for
\end{verbatim}
    \end{minipage}
  \end{center}
  which is easily seen as the program that copies the value of
  \verb|a[n]| in all the coordinates of the vector, as we would have
  expected by transposing the whole matrix :
  \begin{equation*}
    \ell_n = \begin{pmatrix}
      0 & \hdotsfor{3} & 0\\
      \vdots  &  &\vdots&& \vdots \\
      0 & \hdotsfor{3} & 0\\
      1 & \hdotsfor{3} & 1
    \end{pmatrix}
  \end{equation*}
  
  It is true in general that, whenever we have a \verb|for| loop
  running over a variable in the parameter space, the transposition of
  the program is a new \verb|for| loop running backwards over the
  variable and whose body has been transposed. Similar rules apply for
  the other classic constructions of programming languages, thus
  allowing us to semi-automatically transpose programs.

  Whether or not it is possible to fully automate the process of
  transposing a program is an open problem. A better understanding of
  how to identify the better (or at least a good) parameter space
  is certainly a fundamental step to give an answer.


  
  \begin{thebibliography}{1}
    
  \bibitem{BCS}P.~BÃ¼rgisser, M.~Clausen \& M.~A.~Shokrollahi
    \emph{Algebraic Complexity Theory}.
    Springer, 1997.
    
  \bibitem{BLS03}A.~Bostan, G.~Lecerf \& E.~Schost,
    Tellegen's Principle into Practice.
    \emph{Proceedings of ISAAC 2003}.

  \bibitem{WIKI}Wikipedia, The free encycolpedia.
  
  \end{thebibliography}

\end{document}


% Local Variables:
% mode:flyspell
% ispell-local-dictionary:"british"
% End:

% LocalWords:  Tellegen Tellegen's covariant functor contravariant injective
% LocalWords:  coalgorithm cocomplexity bialgorithm subcategory unitaries
